{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart-Pole simulation and Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from env import CartPole, Physics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "def initialize_mdp_data(num_states):\n",
    "    \"\"\"\n",
    "    Taking number of states as parameter and initialize mdp data\n",
    "    \"\"\"\n",
    "    # index for probability: (state now, state after, action)\n",
    "    transition_counts = np.zeros((num_states, num_states, 2))\n",
    "    transition_probs = np.ones((num_states, num_states, 2)) / num_states\n",
    "    # Index zero is count of rewards being -1 , index 1 is count of total num state is reached\n",
    "    reward_counts = np.zeros((num_states, 2))\n",
    "    reward = np.zeros(num_states)\n",
    "    value = np.random.rand(num_states) * 0.1\n",
    "\n",
    "    return {\n",
    "        'transition_counts': transition_counts,\n",
    "        'transition_probs': transition_probs,\n",
    "        'reward_counts': reward_counts,\n",
    "        'reward': reward,\n",
    "        'value': value,\n",
    "        'num_states': num_states,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate best action according to current state\n",
    "- brute force search: loop over all *states* and *action* \n",
    "- more elegant way to do this: **dot product V and P_sa**\n",
    "- Analysis: V - (1 * num_states), P_sa[state] - (num_states * 2) -> endup having array with dimension (1,2), every entry being payoff if taking corresponding action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, mdp_data):\n",
    "    expect_value = mdp_data['value'] @ mdp_data['transition_probs'][state]\n",
    "    if expect_value[0] == expect_value[1]:\n",
    "        return np.random.randint(2) # randomly choose an action\n",
    "    return np.argmax(expect_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Record transition counts and reward counts for update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, reward):\n",
    "    \"\"\"\n",
    "    record reward and state transition counts\n",
    "    \"\"\"\n",
    "    # record number of times reward -1 is seen\n",
    "    if reward == -1:\n",
    "        mdp_data['reward_counts'][new_state][0] += 1\n",
    "    mdp_data['reward_counts'][new_state][1] += 1 # total time for reward\n",
    "    mdp_data['transition_counts'][state][new_state][action] += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actually update transition probability after one iteration\n",
    "- recall update of P_sa need (#took action a in state s) as denominator and (#~got to state s_prime) as numerator\n",
    "- Have P_sa[state][new_state][action] stored, so **sum over new_state** to get denominator(which is sum in axis=1)\n",
    "- Need to perform update for all new_state => **leave index for new_state as slicing**\n",
    "- NOTICE: **when slicing the array, must use comma instead of bracket notation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detailed explanation of difference between bracket and comma notation\n",
    "- For example x = np.array([1,2], [3,4], [5,6])\n",
    "x[:][1] = array([3,4]), means take all indices of x, **then take index 1 along the first axis of the result**, namely the **brackets apply in sequential order**\n",
    "x[:, 1] = array([2,4,6]), means take all indices of x along the first axis, but only index 1 along the second, which is the proper way for matrix slicing\n",
    "- x[1][1] and x[1, 1] are only equivalent because **indexing array with integer shifts all remaining axes towards the front of the shape**, so the first axis of x[1] is the second axis of x, but slicing don't make the shift\n",
    "- ***Should always use commas instead of multiple indexing steps***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mdp_transition_probs_reward(mdp_data):\n",
    "    \"\"\"\n",
    "    Update transition probability after one iteration\n",
    "    \"\"\"\n",
    "    transition_counts = mdp_data['transition_counts']\n",
    "    num_states = mdp_data['num_states']\n",
    "    # times took action in state - sum up new_state\n",
    "    num_counts = transition_counts.sum(axis=1)\n",
    "    # update P_sa = # get to new state / # take action in state\n",
    "    for s in range(num_states):\n",
    "        for action in range(2): # action set being [0,1]\n",
    "            if num_counts[s][action]:\n",
    "                # we have taken action in state s\n",
    "                mdp_data['transition_probs'][s, :, action] = transition_counts[s, :, action] / num_counts[s, action]\n",
    "    reward_counts = mdp_data['reward_counts']\n",
    "    for i in range(num_states):\n",
    "        sum_count = reward_counts[i, 1] # first index being times to reach reward\n",
    "        if sum_count: # this state has reward assigned \n",
    "            mdp_data['reward'][i] = -reward_counts[i, 0] / sum_count\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value iteration for solving Bellman's equation\n",
    "- brute force way: loop over all possible states and actions, obtain new entries one by one\n",
    "- elegant way: value function **dot product** transition probs and **find max along action axis**\n",
    "- Additionally for convergence criterion, instead of using all() for all entry, just **compare the max change with tolerance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mdp_value(mdp_data, tolerance, gamma):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp_data: tuple of states and reward\n",
    "    tolerance: threshold for convergence determination\n",
    "    gamma: discount factor\n",
    "    Returns\n",
    "    -------\n",
    "    whether value iteration converged in one iteration\n",
    "    \"\"\"\n",
    "    V = mdp_data['value']\n",
    "    P_sa = mdp_data['transition_probs']\n",
    "    counter = 0 # record the number of iterations\n",
    "    while True:\n",
    "        counter += 1\n",
    "        # store value function for synchronous update\n",
    "        curr_update = mdp_data['reward'] + gamma * (V @ P_sa).max(axis=1) # before max, shape = (t,2)\n",
    "        if max(abs(V - curr_update)) < tolerance:\n",
    "            break\n",
    "        V = curr_update\n",
    "    mdp_data['value'] = V\n",
    "    return counter == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(plot=True):\n",
    "    # Seed the randomness of the simulation so this outputs the same thing each time\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Simulation parameters\n",
    "    pause_time = 0.0001\n",
    "    min_trial_length_to_start_display = 100\n",
    "    display_started = min_trial_length_to_start_display == 0\n",
    "\n",
    "    NUM_STATES = 163\n",
    "    GAMMA = 0.995\n",
    "    TOLERANCE = 0.01\n",
    "    NO_LEARNING_THRESHOLD = 20\n",
    "\n",
    "    # Time cycle of the simulation\n",
    "    time = 0\n",
    "\n",
    "    # These variables perform bookkeeping (how many cycles was the pole\n",
    "    # balanced for before it fell). Useful for plotting learning curves.\n",
    "    time_steps_to_failure = []\n",
    "    num_failures = 0\n",
    "    time_at_start_of_current_trial = 0\n",
    "\n",
    "    # You should reach convergence well before this\n",
    "    max_failures = 500\n",
    "\n",
    "    # Initialize a cart pole\n",
    "    cart_pole = CartPole(Physics())\n",
    "    # starting simulation with state tuple being all zero\n",
    "    x, x_dot, theta, theta_dot = 0.0, 0.0, 0.0, 0.0\n",
    "    state_tuple = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    state = cart_pole.get_state(state_tuple) # get discrete representation\n",
    "    \"\"\"Display Control\"\"\"\n",
    "    if min_trial_length_to_start_display == 0 or display_started == 1:\n",
    "        cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "    mdp_data = initialize_mdp_data(NUM_STATES)\n",
    "\n",
    "    # convergence criterion: several iteration converge in one flop\n",
    "    consecutive_no_learning_trials = 0\n",
    "    while consecutive_no_learning_trials < NO_LEARNING_THRESHOLD:\n",
    "        action = choose_action(state, mdp_data)\n",
    "        # Get the next state by simulating the dynamics\n",
    "        state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "        # increment simulation time\n",
    "        time += 1\n",
    "        # Get the state number corresponding to new state vector\n",
    "        new_state = cart_pole.get_state(state_tuple)\n",
    "        \"\"\"Display Control\"\"\"\n",
    "        if display_started == 1:\n",
    "            cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "        # specify reward function - -1 for fail, 0 for other\n",
    "        R = -1 if new_state == NUM_STATES - 1 else 0\n",
    "        update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, R)\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            # update transition probs whenever fails\n",
    "            update_mdp_transition_probs_reward(mdp_data)\n",
    "            converged_in_one_iteration = update_mdp_value(mdp_data, TOLERANCE, GAMMA)\n",
    "            if converged_in_one_iteration:\n",
    "                consecutive_no_learning_trials += 1\n",
    "            else:\n",
    "                consecutive_no_learning_trials = 0 # reset\n",
    "\n",
    "        # If fail, reinitialize the states\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            num_failures += 1\n",
    "            if num_failures >= max_failures:\n",
    "                break\n",
    "            print(f'[INFO] Failure number {num_failures}')\n",
    "            time_steps_to_failure.append(time - time_at_start_of_current_trial)\n",
    "            # time_steps_to_failure[num_failures] = time - time_at_start_of_current_trial\n",
    "            time_at_start_of_current_trial = time\n",
    "\n",
    "            if time_steps_to_failure[num_failures - 1] > min_trial_length_to_start_display:\n",
    "                display_started = 1\n",
    "            # Reinitialize state\n",
    "            x = -1.1 + np.random.uniform() * 2.2\n",
    "            x_dot, theta, theta_dot = 0.0, 0.0, 0.0\n",
    "            state_tuple = (x, x_dot, theta, theta_dot)\n",
    "            state = cart_pole.get_state(state_tuple)\n",
    "        else: # update current state\n",
    "            state = new_state\n",
    "    # \"\"\"Display Control\"\"\"\n",
    "    # # after convergence do one more simulation for state showing\n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         state = cart_pole.get_state(state_tuple)\n",
    "    #         action = choose_action(state, mdp_data)\n",
    "    #         state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "    #         new_state = cart_pole.get_state(state_tuple)\n",
    "    #         cart_pole.show_cart(state_tuple, pause_time)\n",
    "    #         if new_state == NUM_STATES - 1:\n",
    "    #             # failure for current trial\n",
    "    #             break\n",
    "    #     except KeyboardInterrupt:\n",
    "    #         break\n",
    "    # plt.show()\n",
    "\n",
    "    if plot:\n",
    "        # plot the learning curve (time balanced vs. trial)\n",
    "        log_tstf = np.log(np.array(time_steps_to_failure))\n",
    "        plt.plot(np.arange(len(time_steps_to_failure)), log_tstf, 'k')\n",
    "        window = 30\n",
    "        w = np.array([1/window for _ in range(window)])\n",
    "        weights = lfilter(w, 1, log_tstf)\n",
    "        x = np.arange(window//2, len(log_tstf) - window//2)\n",
    "        plt.plot(x, weights[window:len(log_tstf)], 'r--')\n",
    "        plt.xlabel('Num failures')\n",
    "        plt.ylabel('Log of num steps to failure')\n",
    "        plt.title('seed = {}'.format(seed))\n",
    "    plt.show()\n",
    "    return np.array(time_steps_to_failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling main() for simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
